{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNOkYoSbwMqX1mYX1esfG7W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhil4674/Imitation-Learning/blob/main/Nvidia_GTC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KPckxVi-4xPe",
        "outputId": "14a771ae-2fdb-4dba-aa3d-971a2cb88786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Transition Loss: 0.3361, Policy Loss: 0.2376\n",
            "Epoch 2, Transition Loss: 0.4149, Policy Loss: 0.1259\n",
            "Epoch 3, Transition Loss: 0.2296, Policy Loss: 0.2165\n",
            "Epoch 4, Transition Loss: 0.2078, Policy Loss: 0.1064\n",
            "Epoch 5, Transition Loss: 0.1527, Policy Loss: 0.0908\n",
            "Epoch 6, Transition Loss: 0.1745, Policy Loss: 0.1179\n",
            "Epoch 7, Transition Loss: 0.1574, Policy Loss: 0.0963\n",
            "Epoch 8, Transition Loss: 0.1265, Policy Loss: 0.0722\n",
            "Epoch 9, Transition Loss: 0.1014, Policy Loss: 0.0874\n",
            "Epoch 10, Transition Loss: 0.1416, Policy Loss: 0.1390\n",
            "Epoch 11, Transition Loss: 0.0693, Policy Loss: 0.1076\n",
            "Epoch 12, Transition Loss: 0.1392, Policy Loss: 0.0734\n",
            "Epoch 13, Transition Loss: 0.0914, Policy Loss: 0.1302\n",
            "Epoch 14, Transition Loss: 0.0704, Policy Loss: 0.0914\n",
            "Epoch 15, Transition Loss: 0.0976, Policy Loss: 0.0632\n",
            "Epoch 16, Transition Loss: 0.1229, Policy Loss: 0.0577\n",
            "Epoch 17, Transition Loss: 0.0680, Policy Loss: 0.0805\n",
            "Epoch 18, Transition Loss: 0.1024, Policy Loss: 0.0776\n",
            "Epoch 19, Transition Loss: 0.0801, Policy Loss: 0.0456\n",
            "Epoch 20, Transition Loss: 0.1203, Policy Loss: 0.0599\n",
            "Epoch 21, Transition Loss: 0.0763, Policy Loss: 0.0741\n",
            "Epoch 22, Transition Loss: 0.0747, Policy Loss: 0.0508\n",
            "Epoch 23, Transition Loss: 0.0762, Policy Loss: 0.0948\n",
            "Epoch 24, Transition Loss: 0.0869, Policy Loss: 0.0373\n",
            "Epoch 25, Transition Loss: 0.0765, Policy Loss: 0.0768\n",
            "Epoch 26, Transition Loss: 0.0529, Policy Loss: 0.0631\n",
            "Epoch 27, Transition Loss: 0.0548, Policy Loss: 0.0804\n",
            "Epoch 28, Transition Loss: 0.1272, Policy Loss: 0.0648\n",
            "Epoch 29, Transition Loss: 0.1282, Policy Loss: 0.0758\n",
            "Epoch 30, Transition Loss: 0.0912, Policy Loss: 0.0539\n",
            "Epoch 31, Transition Loss: 0.0856, Policy Loss: 0.0851\n",
            "Epoch 32, Transition Loss: 0.0879, Policy Loss: 0.0773\n",
            "Epoch 33, Transition Loss: 0.0874, Policy Loss: 0.1107\n",
            "Epoch 34, Transition Loss: 0.0794, Policy Loss: 0.0785\n",
            "Epoch 35, Transition Loss: 0.0684, Policy Loss: 0.0855\n",
            "Epoch 36, Transition Loss: 0.0904, Policy Loss: 0.1136\n",
            "Epoch 37, Transition Loss: 0.0897, Policy Loss: 0.1211\n",
            "Epoch 38, Transition Loss: 0.1074, Policy Loss: 0.0573\n",
            "Epoch 39, Transition Loss: 0.0909, Policy Loss: 0.0916\n",
            "Epoch 40, Transition Loss: 0.0819, Policy Loss: 0.0374\n",
            "Epoch 41, Transition Loss: 0.0725, Policy Loss: 0.1195\n",
            "Epoch 42, Transition Loss: 0.0782, Policy Loss: 0.0476\n",
            "Epoch 43, Transition Loss: 0.0867, Policy Loss: 0.0680\n",
            "Epoch 44, Transition Loss: 0.1119, Policy Loss: 0.0922\n",
            "Epoch 45, Transition Loss: 0.0882, Policy Loss: 0.1139\n",
            "Epoch 46, Transition Loss: 0.0718, Policy Loss: 0.0472\n",
            "Epoch 47, Transition Loss: 0.0832, Policy Loss: 0.1163\n",
            "Epoch 48, Transition Loss: 0.0848, Policy Loss: 0.0966\n",
            "Epoch 49, Transition Loss: 0.0583, Policy Loss: 0.1060\n",
            "Epoch 50, Transition Loss: 0.0792, Policy Loss: 0.0169\n",
            "Epoch 51, Transition Loss: 0.0741, Policy Loss: 0.0624\n",
            "Epoch 52, Transition Loss: 0.0853, Policy Loss: 0.0667\n",
            "Epoch 53, Transition Loss: 0.1058, Policy Loss: 0.0638\n",
            "Epoch 54, Transition Loss: 0.1043, Policy Loss: 0.0950\n",
            "Epoch 55, Transition Loss: 0.0865, Policy Loss: 0.0800\n",
            "Epoch 56, Transition Loss: 0.0445, Policy Loss: 0.0670\n",
            "Epoch 57, Transition Loss: 0.0763, Policy Loss: 0.0634\n",
            "Epoch 58, Transition Loss: 0.0884, Policy Loss: 0.0634\n",
            "Epoch 59, Transition Loss: 0.0796, Policy Loss: 0.0723\n",
            "Epoch 60, Transition Loss: 0.0662, Policy Loss: 0.0726\n",
            "Epoch 61, Transition Loss: 0.0679, Policy Loss: 0.1092\n",
            "Epoch 62, Transition Loss: 0.0905, Policy Loss: 0.0994\n",
            "Epoch 63, Transition Loss: 0.0612, Policy Loss: 0.0660\n",
            "Epoch 64, Transition Loss: 0.1030, Policy Loss: 0.0685\n",
            "Epoch 65, Transition Loss: 0.0846, Policy Loss: 0.0781\n",
            "Epoch 66, Transition Loss: 0.1293, Policy Loss: 0.0605\n",
            "Epoch 67, Transition Loss: 0.1173, Policy Loss: 0.0645\n",
            "Epoch 68, Transition Loss: 0.0714, Policy Loss: 0.0517\n",
            "Epoch 69, Transition Loss: 0.0753, Policy Loss: 0.1074\n",
            "Epoch 70, Transition Loss: 0.0853, Policy Loss: 0.1322\n",
            "Epoch 71, Transition Loss: 0.0861, Policy Loss: 0.0501\n",
            "Epoch 72, Transition Loss: 0.0894, Policy Loss: 0.0674\n",
            "Epoch 73, Transition Loss: 0.0687, Policy Loss: 0.0622\n",
            "Epoch 74, Transition Loss: 0.0686, Policy Loss: 0.0839\n",
            "Epoch 75, Transition Loss: 0.0576, Policy Loss: 0.0903\n",
            "Epoch 76, Transition Loss: 0.0944, Policy Loss: 0.0753\n",
            "Epoch 77, Transition Loss: 0.0855, Policy Loss: 0.0848\n",
            "Epoch 78, Transition Loss: 0.0604, Policy Loss: 0.0947\n",
            "Epoch 79, Transition Loss: 0.0743, Policy Loss: 0.0737\n",
            "Epoch 80, Transition Loss: 0.1016, Policy Loss: 0.1022\n",
            "Epoch 81, Transition Loss: 0.0962, Policy Loss: 0.0614\n",
            "Epoch 82, Transition Loss: 0.0708, Policy Loss: 0.0601\n",
            "Epoch 83, Transition Loss: 0.1245, Policy Loss: 0.0297\n",
            "Epoch 84, Transition Loss: 0.0883, Policy Loss: 0.0311\n",
            "Epoch 85, Transition Loss: 0.0770, Policy Loss: 0.0374\n",
            "Epoch 86, Transition Loss: 0.0774, Policy Loss: 0.0703\n",
            "Epoch 87, Transition Loss: 0.0633, Policy Loss: 0.0782\n",
            "Epoch 88, Transition Loss: 0.0802, Policy Loss: 0.0779\n",
            "Epoch 89, Transition Loss: 0.0938, Policy Loss: 0.0966\n",
            "Epoch 90, Transition Loss: 0.0962, Policy Loss: 0.0533\n",
            "Epoch 91, Transition Loss: 0.0736, Policy Loss: 0.1049\n",
            "Epoch 92, Transition Loss: 0.0605, Policy Loss: 0.0963\n",
            "Epoch 93, Transition Loss: 0.0656, Policy Loss: 0.0664\n",
            "Epoch 94, Transition Loss: 0.0696, Policy Loss: 0.0596\n",
            "Epoch 95, Transition Loss: 0.0550, Policy Loss: 0.0853\n",
            "Epoch 96, Transition Loss: 0.0534, Policy Loss: 0.0812\n",
            "Epoch 97, Transition Loss: 0.0821, Policy Loss: 0.0528\n",
            "Epoch 98, Transition Loss: 0.0577, Policy Loss: 0.0523\n",
            "Epoch 99, Transition Loss: 0.0485, Policy Loss: 0.0565\n",
            "Epoch 100, Transition Loss: 0.0638, Policy Loss: 0.0829\n",
            "Epoch 101, Transition Loss: 0.1041, Policy Loss: 0.1021\n",
            "Epoch 102, Transition Loss: 0.0536, Policy Loss: 0.0647\n",
            "Epoch 103, Transition Loss: 0.0688, Policy Loss: 0.0826\n",
            "Epoch 104, Transition Loss: 0.0698, Policy Loss: 0.0594\n",
            "Epoch 105, Transition Loss: 0.0994, Policy Loss: 0.0521\n",
            "Epoch 106, Transition Loss: 0.0708, Policy Loss: 0.0374\n",
            "Epoch 107, Transition Loss: 0.0671, Policy Loss: 0.0371\n",
            "Epoch 108, Transition Loss: 0.0538, Policy Loss: 0.0887\n",
            "Epoch 109, Transition Loss: 0.0380, Policy Loss: 0.0907\n",
            "Epoch 110, Transition Loss: 0.0882, Policy Loss: 0.0434\n",
            "Epoch 111, Transition Loss: 0.0551, Policy Loss: 0.0883\n",
            "Epoch 112, Transition Loss: 0.0721, Policy Loss: 0.1192\n",
            "Epoch 113, Transition Loss: 0.1256, Policy Loss: 0.0722\n",
            "Epoch 114, Transition Loss: 0.0748, Policy Loss: 0.0455\n",
            "Epoch 115, Transition Loss: 0.0741, Policy Loss: 0.0719\n",
            "Epoch 116, Transition Loss: 0.0612, Policy Loss: 0.0627\n",
            "Epoch 117, Transition Loss: 0.0683, Policy Loss: 0.1066\n",
            "Epoch 118, Transition Loss: 0.0858, Policy Loss: 0.0484\n",
            "Epoch 119, Transition Loss: 0.0610, Policy Loss: 0.0575\n",
            "Epoch 120, Transition Loss: 0.0859, Policy Loss: 0.0715\n",
            "Epoch 121, Transition Loss: 0.0485, Policy Loss: 0.0670\n",
            "Epoch 122, Transition Loss: 0.0504, Policy Loss: 0.0618\n",
            "Epoch 123, Transition Loss: 0.0983, Policy Loss: 0.0228\n",
            "Epoch 124, Transition Loss: 0.0842, Policy Loss: 0.0799\n",
            "Epoch 125, Transition Loss: 0.0710, Policy Loss: 0.0651\n",
            "Epoch 126, Transition Loss: 0.0755, Policy Loss: 0.0603\n",
            "Epoch 127, Transition Loss: 0.0839, Policy Loss: 0.0948\n",
            "Epoch 128, Transition Loss: 0.0651, Policy Loss: 0.0331\n",
            "Epoch 129, Transition Loss: 0.0848, Policy Loss: 0.0464\n",
            "Epoch 130, Transition Loss: 0.0808, Policy Loss: 0.0879\n",
            "Epoch 131, Transition Loss: 0.0774, Policy Loss: 0.0499\n",
            "Epoch 132, Transition Loss: 0.0835, Policy Loss: 0.1245\n",
            "Epoch 133, Transition Loss: 0.0856, Policy Loss: 0.0882\n",
            "Epoch 134, Transition Loss: 0.0492, Policy Loss: 0.0942\n",
            "Epoch 135, Transition Loss: 0.0622, Policy Loss: 0.0495\n",
            "Epoch 136, Transition Loss: 0.0637, Policy Loss: 0.0318\n",
            "Epoch 137, Transition Loss: 0.0483, Policy Loss: 0.0440\n",
            "Epoch 138, Transition Loss: 0.0525, Policy Loss: 0.0812\n",
            "Epoch 139, Transition Loss: 0.0852, Policy Loss: 0.0850\n",
            "Epoch 140, Transition Loss: 0.1055, Policy Loss: 0.0855\n",
            "Epoch 141, Transition Loss: 0.0970, Policy Loss: 0.1095\n",
            "Epoch 142, Transition Loss: 0.0537, Policy Loss: 0.0505\n",
            "Epoch 143, Transition Loss: 0.0682, Policy Loss: 0.0925\n",
            "Epoch 144, Transition Loss: 0.0952, Policy Loss: 0.0425\n",
            "Epoch 145, Transition Loss: 0.0525, Policy Loss: 0.0966\n",
            "Epoch 146, Transition Loss: 0.0904, Policy Loss: 0.0831\n",
            "Epoch 147, Transition Loss: 0.0738, Policy Loss: 0.0483\n",
            "Epoch 148, Transition Loss: 0.0711, Policy Loss: 0.0743\n",
            "Epoch 149, Transition Loss: 0.0996, Policy Loss: 0.0593\n",
            "Epoch 150, Transition Loss: 0.0633, Policy Loss: 0.0786\n",
            "Epoch 151, Transition Loss: 0.0689, Policy Loss: 0.0601\n",
            "Epoch 152, Transition Loss: 0.0940, Policy Loss: 0.0607\n",
            "Epoch 153, Transition Loss: 0.0673, Policy Loss: 0.1035\n",
            "Epoch 154, Transition Loss: 0.0606, Policy Loss: 0.0927\n",
            "Epoch 155, Transition Loss: 0.0759, Policy Loss: 0.0589\n",
            "Epoch 156, Transition Loss: 0.1143, Policy Loss: 0.0818\n",
            "Epoch 157, Transition Loss: 0.0734, Policy Loss: 0.0762\n",
            "Epoch 158, Transition Loss: 0.0676, Policy Loss: 0.0413\n",
            "Epoch 159, Transition Loss: 0.1001, Policy Loss: 0.0719\n",
            "Epoch 160, Transition Loss: 0.0463, Policy Loss: 0.0707\n",
            "Epoch 161, Transition Loss: 0.0778, Policy Loss: 0.0592\n",
            "Epoch 162, Transition Loss: 0.0677, Policy Loss: 0.0740\n",
            "Epoch 163, Transition Loss: 0.0588, Policy Loss: 0.0432\n",
            "Epoch 164, Transition Loss: 0.0673, Policy Loss: 0.0516\n",
            "Epoch 165, Transition Loss: 0.0519, Policy Loss: 0.0932\n",
            "Epoch 166, Transition Loss: 0.1138, Policy Loss: 0.0304\n",
            "Epoch 167, Transition Loss: 0.0598, Policy Loss: 0.0735\n",
            "Epoch 168, Transition Loss: 0.0478, Policy Loss: 0.0446\n",
            "Epoch 169, Transition Loss: 0.0654, Policy Loss: 0.0712\n",
            "Epoch 170, Transition Loss: 0.0765, Policy Loss: 0.0521\n",
            "Epoch 171, Transition Loss: 0.0561, Policy Loss: 0.0436\n",
            "Epoch 172, Transition Loss: 0.0636, Policy Loss: 0.0283\n",
            "Epoch 173, Transition Loss: 0.0782, Policy Loss: 0.0520\n",
            "Epoch 174, Transition Loss: 0.0652, Policy Loss: 0.0609\n",
            "Epoch 175, Transition Loss: 0.0692, Policy Loss: 0.0702\n",
            "Epoch 176, Transition Loss: 0.0714, Policy Loss: 0.0958\n",
            "Epoch 177, Transition Loss: 0.0680, Policy Loss: 0.0447\n",
            "Epoch 178, Transition Loss: 0.0720, Policy Loss: 0.0955\n",
            "Epoch 179, Transition Loss: 0.0755, Policy Loss: 0.0789\n",
            "Epoch 180, Transition Loss: 0.0525, Policy Loss: 0.0735\n",
            "Epoch 181, Transition Loss: 0.0783, Policy Loss: 0.0485\n",
            "Epoch 182, Transition Loss: 0.0737, Policy Loss: 0.0248\n",
            "Epoch 183, Transition Loss: 0.0522, Policy Loss: 0.0756\n",
            "Epoch 184, Transition Loss: 0.0545, Policy Loss: 0.0597\n",
            "Epoch 185, Transition Loss: 0.0797, Policy Loss: 0.0707\n",
            "Epoch 186, Transition Loss: 0.0733, Policy Loss: 0.0542\n",
            "Epoch 187, Transition Loss: 0.0894, Policy Loss: 0.0648\n",
            "Epoch 188, Transition Loss: 0.0672, Policy Loss: 0.0564\n",
            "Epoch 189, Transition Loss: 0.0754, Policy Loss: 0.0616\n",
            "Epoch 190, Transition Loss: 0.1077, Policy Loss: 0.0820\n",
            "Epoch 191, Transition Loss: 0.0848, Policy Loss: 0.0619\n",
            "Epoch 192, Transition Loss: 0.0779, Policy Loss: 0.0553\n",
            "Epoch 193, Transition Loss: 0.0845, Policy Loss: 0.0363\n",
            "Epoch 194, Transition Loss: 0.0754, Policy Loss: 0.0659\n",
            "Epoch 195, Transition Loss: 0.0477, Policy Loss: 0.0794\n",
            "Epoch 196, Transition Loss: 0.0463, Policy Loss: 0.0426\n",
            "Epoch 197, Transition Loss: 0.1137, Policy Loss: 0.0757\n",
            "Epoch 198, Transition Loss: 0.0713, Policy Loss: 0.0579\n",
            "Epoch 199, Transition Loss: 0.0403, Policy Loss: 0.1138\n",
            "Epoch 200, Transition Loss: 0.0650, Policy Loss: 0.0748\n",
            "Epoch 201, Transition Loss: 0.0872, Policy Loss: 0.0831\n",
            "Epoch 202, Transition Loss: 0.0503, Policy Loss: 0.0758\n",
            "Epoch 203, Transition Loss: 0.0530, Policy Loss: 0.0913\n",
            "Epoch 204, Transition Loss: 0.0406, Policy Loss: 0.0841\n",
            "Epoch 205, Transition Loss: 0.0595, Policy Loss: 0.0541\n",
            "Epoch 206, Transition Loss: 0.0988, Policy Loss: 0.0314\n",
            "Epoch 207, Transition Loss: 0.0503, Policy Loss: 0.1034\n",
            "Epoch 208, Transition Loss: 0.0546, Policy Loss: 0.0556\n",
            "Epoch 209, Transition Loss: 0.0823, Policy Loss: 0.0567\n",
            "Epoch 210, Transition Loss: 0.0578, Policy Loss: 0.0901\n",
            "Epoch 211, Transition Loss: 0.1067, Policy Loss: 0.0434\n",
            "Epoch 212, Transition Loss: 0.0630, Policy Loss: 0.0977\n",
            "Epoch 213, Transition Loss: 0.0638, Policy Loss: 0.1119\n",
            "Epoch 214, Transition Loss: 0.0239, Policy Loss: 0.0708\n",
            "Epoch 215, Transition Loss: 0.0586, Policy Loss: 0.0633\n",
            "Epoch 216, Transition Loss: 0.0437, Policy Loss: 0.0866\n",
            "Epoch 217, Transition Loss: 0.0684, Policy Loss: 0.0570\n",
            "Epoch 218, Transition Loss: 0.0732, Policy Loss: 0.0558\n",
            "Epoch 219, Transition Loss: 0.0521, Policy Loss: 0.0666\n",
            "Epoch 220, Transition Loss: 0.0630, Policy Loss: 0.0634\n",
            "Epoch 221, Transition Loss: 0.0724, Policy Loss: 0.0736\n",
            "Epoch 222, Transition Loss: 0.0560, Policy Loss: 0.0547\n",
            "Epoch 223, Transition Loss: 0.0605, Policy Loss: 0.0815\n",
            "Epoch 224, Transition Loss: 0.1089, Policy Loss: 0.0971\n",
            "Epoch 225, Transition Loss: 0.0505, Policy Loss: 0.0897\n",
            "Epoch 226, Transition Loss: 0.0773, Policy Loss: 0.0575\n",
            "Epoch 227, Transition Loss: 0.0560, Policy Loss: 0.0514\n",
            "Epoch 228, Transition Loss: 0.0936, Policy Loss: 0.0837\n",
            "Epoch 229, Transition Loss: 0.0952, Policy Loss: 0.0382\n",
            "Epoch 230, Transition Loss: 0.0737, Policy Loss: 0.0806\n",
            "Epoch 231, Transition Loss: 0.0581, Policy Loss: 0.0763\n",
            "Epoch 232, Transition Loss: 0.0824, Policy Loss: 0.0524\n",
            "Epoch 233, Transition Loss: 0.0631, Policy Loss: 0.0893\n",
            "Epoch 234, Transition Loss: 0.0565, Policy Loss: 0.0467\n",
            "Epoch 235, Transition Loss: 0.0712, Policy Loss: 0.0459\n",
            "Epoch 236, Transition Loss: 0.0553, Policy Loss: 0.0651\n",
            "Epoch 237, Transition Loss: 0.0656, Policy Loss: 0.0625\n",
            "Epoch 238, Transition Loss: 0.0824, Policy Loss: 0.0792\n",
            "Epoch 239, Transition Loss: 0.0742, Policy Loss: 0.0705\n",
            "Epoch 240, Transition Loss: 0.0874, Policy Loss: 0.0742\n",
            "Epoch 241, Transition Loss: 0.0979, Policy Loss: 0.0809\n",
            "Epoch 242, Transition Loss: 0.0766, Policy Loss: 0.0990\n",
            "Epoch 243, Transition Loss: 0.0873, Policy Loss: 0.0229\n",
            "Epoch 244, Transition Loss: 0.0733, Policy Loss: 0.0393\n",
            "Epoch 245, Transition Loss: 0.0501, Policy Loss: 0.0832\n",
            "Epoch 246, Transition Loss: 0.0739, Policy Loss: 0.0763\n",
            "Epoch 247, Transition Loss: 0.0482, Policy Loss: 0.0485\n",
            "Epoch 248, Transition Loss: 0.0273, Policy Loss: 0.0726\n",
            "Epoch 249, Transition Loss: 0.0835, Policy Loss: 0.0700\n",
            "Epoch 250, Transition Loss: 0.0750, Policy Loss: 0.0838\n",
            "Epoch 251, Transition Loss: 0.0687, Policy Loss: 0.0620\n",
            "Epoch 252, Transition Loss: 0.0540, Policy Loss: 0.0730\n",
            "Epoch 253, Transition Loss: 0.0677, Policy Loss: 0.0536\n",
            "Epoch 254, Transition Loss: 0.0849, Policy Loss: 0.0470\n",
            "Epoch 255, Transition Loss: 0.0593, Policy Loss: 0.0356\n",
            "Epoch 256, Transition Loss: 0.0488, Policy Loss: 0.0838\n",
            "Epoch 257, Transition Loss: 0.0610, Policy Loss: 0.0889\n",
            "Epoch 258, Transition Loss: 0.0798, Policy Loss: 0.0728\n",
            "Epoch 259, Transition Loss: 0.0705, Policy Loss: 0.0469\n",
            "Epoch 260, Transition Loss: 0.0464, Policy Loss: 0.0646\n",
            "Epoch 261, Transition Loss: 0.0541, Policy Loss: 0.0881\n",
            "Epoch 262, Transition Loss: 0.0796, Policy Loss: 0.0632\n",
            "Epoch 263, Transition Loss: 0.1023, Policy Loss: 0.0658\n",
            "Epoch 264, Transition Loss: 0.0538, Policy Loss: 0.0975\n",
            "Epoch 265, Transition Loss: 0.0748, Policy Loss: 0.0823\n",
            "Epoch 266, Transition Loss: 0.0575, Policy Loss: 0.0445\n",
            "Epoch 267, Transition Loss: 0.0695, Policy Loss: 0.0606\n",
            "Epoch 268, Transition Loss: 0.0878, Policy Loss: 0.0606\n",
            "Epoch 269, Transition Loss: 0.0618, Policy Loss: 0.0653\n",
            "Epoch 270, Transition Loss: 0.0602, Policy Loss: 0.0781\n",
            "Epoch 271, Transition Loss: 0.1075, Policy Loss: 0.0465\n",
            "Epoch 272, Transition Loss: 0.0325, Policy Loss: 0.0355\n",
            "Epoch 273, Transition Loss: 0.0859, Policy Loss: 0.0208\n",
            "Epoch 274, Transition Loss: 0.0764, Policy Loss: 0.0725\n",
            "Epoch 275, Transition Loss: 0.0535, Policy Loss: 0.0896\n",
            "Epoch 276, Transition Loss: 0.0747, Policy Loss: 0.0645\n",
            "Epoch 277, Transition Loss: 0.0445, Policy Loss: 0.0526\n",
            "Epoch 278, Transition Loss: 0.0793, Policy Loss: 0.0572\n",
            "Epoch 279, Transition Loss: 0.0895, Policy Loss: 0.0701\n",
            "Epoch 280, Transition Loss: 0.0716, Policy Loss: 0.0456\n",
            "Epoch 281, Transition Loss: 0.0470, Policy Loss: 0.0453\n",
            "Epoch 282, Transition Loss: 0.0596, Policy Loss: 0.0956\n",
            "Epoch 283, Transition Loss: 0.0559, Policy Loss: 0.0894\n",
            "Epoch 284, Transition Loss: 0.0598, Policy Loss: 0.0597\n",
            "Epoch 285, Transition Loss: 0.0543, Policy Loss: 0.0513\n",
            "Epoch 286, Transition Loss: 0.0833, Policy Loss: 0.0593\n",
            "Epoch 287, Transition Loss: 0.0588, Policy Loss: 0.0422\n",
            "Epoch 288, Transition Loss: 0.0568, Policy Loss: 0.0827\n",
            "Epoch 289, Transition Loss: 0.0608, Policy Loss: 0.0556\n",
            "Epoch 290, Transition Loss: 0.0726, Policy Loss: 0.0527\n",
            "Epoch 291, Transition Loss: 0.0821, Policy Loss: 0.0423\n",
            "Epoch 292, Transition Loss: 0.0563, Policy Loss: 0.0352\n",
            "Epoch 293, Transition Loss: 0.0450, Policy Loss: 0.0779\n",
            "Epoch 294, Transition Loss: 0.0701, Policy Loss: 0.0583\n",
            "Epoch 295, Transition Loss: 0.0802, Policy Loss: 0.0606\n",
            "Epoch 296, Transition Loss: 0.0604, Policy Loss: 0.0346\n",
            "Epoch 297, Transition Loss: 0.0575, Policy Loss: 0.0780\n",
            "Epoch 298, Transition Loss: 0.0478, Policy Loss: 0.1346\n",
            "Epoch 299, Transition Loss: 0.0793, Policy Loss: 0.0317\n",
            "Epoch 300, Transition Loss: 0.1080, Policy Loss: 0.0554\n",
            "Epoch 301, Transition Loss: 0.0854, Policy Loss: 0.0488\n",
            "Epoch 302, Transition Loss: 0.0616, Policy Loss: 0.0399\n",
            "Epoch 303, Transition Loss: 0.0666, Policy Loss: 0.0324\n",
            "Epoch 304, Transition Loss: 0.0420, Policy Loss: 0.0858\n",
            "Epoch 305, Transition Loss: 0.0477, Policy Loss: 0.0403\n",
            "Epoch 306, Transition Loss: 0.0762, Policy Loss: 0.0945\n",
            "Epoch 307, Transition Loss: 0.0612, Policy Loss: 0.0267\n",
            "Epoch 308, Transition Loss: 0.0414, Policy Loss: 0.0599\n",
            "Epoch 309, Transition Loss: 0.0572, Policy Loss: 0.0512\n",
            "Epoch 310, Transition Loss: 0.0539, Policy Loss: 0.0470\n",
            "Epoch 311, Transition Loss: 0.0575, Policy Loss: 0.1053\n",
            "Epoch 312, Transition Loss: 0.0504, Policy Loss: 0.0827\n",
            "Epoch 313, Transition Loss: 0.0391, Policy Loss: 0.0619\n",
            "Epoch 314, Transition Loss: 0.0742, Policy Loss: 0.0331\n",
            "Epoch 315, Transition Loss: 0.0457, Policy Loss: 0.0624\n",
            "Epoch 316, Transition Loss: 0.0661, Policy Loss: 0.0931\n",
            "Epoch 317, Transition Loss: 0.0543, Policy Loss: 0.1136\n",
            "Epoch 318, Transition Loss: 0.0524, Policy Loss: 0.0486\n",
            "Epoch 319, Transition Loss: 0.0527, Policy Loss: 0.0549\n",
            "Epoch 320, Transition Loss: 0.0345, Policy Loss: 0.0859\n",
            "Epoch 321, Transition Loss: 0.0778, Policy Loss: 0.0559\n",
            "Epoch 322, Transition Loss: 0.0809, Policy Loss: 0.0596\n",
            "Epoch 323, Transition Loss: 0.0528, Policy Loss: 0.0848\n",
            "Epoch 324, Transition Loss: 0.0475, Policy Loss: 0.0139\n",
            "Epoch 325, Transition Loss: 0.0549, Policy Loss: 0.0708\n",
            "Epoch 326, Transition Loss: 0.0677, Policy Loss: 0.0622\n",
            "Epoch 327, Transition Loss: 0.0618, Policy Loss: 0.0490\n",
            "Epoch 328, Transition Loss: 0.0588, Policy Loss: 0.0334\n",
            "Epoch 329, Transition Loss: 0.0430, Policy Loss: 0.0618\n",
            "Epoch 330, Transition Loss: 0.0496, Policy Loss: 0.0554\n",
            "Epoch 331, Transition Loss: 0.0391, Policy Loss: 0.0834\n",
            "Epoch 332, Transition Loss: 0.0717, Policy Loss: 0.0598\n",
            "Epoch 333, Transition Loss: 0.0873, Policy Loss: 0.0605\n",
            "Epoch 334, Transition Loss: 0.0539, Policy Loss: 0.0685\n",
            "Epoch 335, Transition Loss: 0.0457, Policy Loss: 0.0300\n",
            "Epoch 336, Transition Loss: 0.0499, Policy Loss: 0.0579\n",
            "Epoch 337, Transition Loss: 0.0655, Policy Loss: 0.0798\n",
            "Epoch 338, Transition Loss: 0.0761, Policy Loss: 0.0530\n",
            "Epoch 339, Transition Loss: 0.0478, Policy Loss: 0.0601\n",
            "Epoch 340, Transition Loss: 0.0574, Policy Loss: 0.1000\n",
            "Epoch 341, Transition Loss: 0.0511, Policy Loss: 0.0466\n",
            "Epoch 342, Transition Loss: 0.0470, Policy Loss: 0.0491\n",
            "Epoch 343, Transition Loss: 0.0690, Policy Loss: 0.0365\n",
            "Epoch 344, Transition Loss: 0.0635, Policy Loss: 0.0629\n",
            "Epoch 345, Transition Loss: 0.0962, Policy Loss: 0.0786\n",
            "Epoch 346, Transition Loss: 0.0644, Policy Loss: 0.0577\n",
            "Epoch 347, Transition Loss: 0.0506, Policy Loss: 0.0528\n",
            "Epoch 348, Transition Loss: 0.0588, Policy Loss: 0.0874\n",
            "Epoch 349, Transition Loss: 0.0543, Policy Loss: 0.0187\n",
            "Epoch 350, Transition Loss: 0.0653, Policy Loss: 0.0348\n",
            "Epoch 351, Transition Loss: 0.0681, Policy Loss: 0.0629\n",
            "Epoch 352, Transition Loss: 0.0617, Policy Loss: 0.0692\n",
            "Epoch 353, Transition Loss: 0.0579, Policy Loss: 0.0614\n",
            "Epoch 354, Transition Loss: 0.0709, Policy Loss: 0.0514\n",
            "Epoch 355, Transition Loss: 0.0601, Policy Loss: 0.0787\n",
            "Epoch 356, Transition Loss: 0.0610, Policy Loss: 0.0536\n",
            "Epoch 357, Transition Loss: 0.0671, Policy Loss: 0.1059\n",
            "Epoch 358, Transition Loss: 0.0574, Policy Loss: 0.0460\n",
            "Epoch 359, Transition Loss: 0.0715, Policy Loss: 0.0907\n",
            "Epoch 360, Transition Loss: 0.0610, Policy Loss: 0.0182\n",
            "Epoch 361, Transition Loss: 0.1105, Policy Loss: 0.0926\n",
            "Epoch 362, Transition Loss: 0.0629, Policy Loss: 0.0914\n",
            "Epoch 363, Transition Loss: 0.0658, Policy Loss: 0.0597\n",
            "Epoch 364, Transition Loss: 0.0506, Policy Loss: 0.0673\n",
            "Epoch 365, Transition Loss: 0.0504, Policy Loss: 0.0683\n",
            "Epoch 366, Transition Loss: 0.0701, Policy Loss: 0.0805\n",
            "Epoch 367, Transition Loss: 0.0549, Policy Loss: 0.0712\n",
            "Epoch 368, Transition Loss: 0.0825, Policy Loss: 0.0549\n",
            "Epoch 369, Transition Loss: 0.0511, Policy Loss: 0.0731\n",
            "Epoch 370, Transition Loss: 0.0363, Policy Loss: 0.0400\n",
            "Epoch 371, Transition Loss: 0.0655, Policy Loss: 0.0635\n",
            "Epoch 372, Transition Loss: 0.0470, Policy Loss: 0.0839\n",
            "Epoch 373, Transition Loss: 0.0584, Policy Loss: 0.0358\n",
            "Epoch 374, Transition Loss: 0.0486, Policy Loss: 0.0537\n",
            "Epoch 375, Transition Loss: 0.0418, Policy Loss: 0.1147\n",
            "Epoch 376, Transition Loss: 0.0560, Policy Loss: 0.0308\n",
            "Epoch 377, Transition Loss: 0.0646, Policy Loss: 0.0688\n",
            "Epoch 378, Transition Loss: 0.0773, Policy Loss: 0.0475\n",
            "Epoch 379, Transition Loss: 0.0604, Policy Loss: 0.0627\n",
            "Epoch 380, Transition Loss: 0.0596, Policy Loss: 0.0484\n",
            "Epoch 381, Transition Loss: 0.0942, Policy Loss: 0.0669\n",
            "Epoch 382, Transition Loss: 0.0591, Policy Loss: 0.0594\n",
            "Epoch 383, Transition Loss: 0.0473, Policy Loss: 0.0967\n",
            "Epoch 384, Transition Loss: 0.0730, Policy Loss: 0.0679\n",
            "Epoch 385, Transition Loss: 0.0388, Policy Loss: 0.0572\n",
            "Epoch 386, Transition Loss: 0.0572, Policy Loss: 0.0597\n",
            "Epoch 387, Transition Loss: 0.0808, Policy Loss: 0.0994\n",
            "Epoch 388, Transition Loss: 0.0634, Policy Loss: 0.0951\n",
            "Epoch 389, Transition Loss: 0.1009, Policy Loss: 0.0257\n",
            "Epoch 390, Transition Loss: 0.0537, Policy Loss: 0.0612\n",
            "Epoch 391, Transition Loss: 0.0545, Policy Loss: 0.0642\n",
            "Epoch 392, Transition Loss: 0.0590, Policy Loss: 0.0666\n",
            "Epoch 393, Transition Loss: 0.0582, Policy Loss: 0.0692\n",
            "Epoch 394, Transition Loss: 0.0895, Policy Loss: 0.0553\n",
            "Epoch 395, Transition Loss: 0.0704, Policy Loss: 0.0512\n",
            "Epoch 396, Transition Loss: 0.0624, Policy Loss: 0.0663\n",
            "Epoch 397, Transition Loss: 0.0649, Policy Loss: 0.0500\n",
            "Epoch 398, Transition Loss: 0.0418, Policy Loss: 0.0458\n",
            "Epoch 399, Transition Loss: 0.0794, Policy Loss: 0.0290\n",
            "Epoch 400, Transition Loss: 0.0531, Policy Loss: 0.1343\n",
            "Epoch 401, Transition Loss: 0.0694, Policy Loss: 0.0888\n",
            "Epoch 402, Transition Loss: 0.0784, Policy Loss: 0.0773\n",
            "Epoch 403, Transition Loss: 0.0532, Policy Loss: 0.0566\n",
            "Epoch 404, Transition Loss: 0.0554, Policy Loss: 0.0443\n",
            "Epoch 405, Transition Loss: 0.0526, Policy Loss: 0.0413\n",
            "Epoch 406, Transition Loss: 0.0357, Policy Loss: 0.0431\n",
            "Epoch 407, Transition Loss: 0.0419, Policy Loss: 0.0255\n",
            "Epoch 408, Transition Loss: 0.0486, Policy Loss: 0.0312\n",
            "Epoch 409, Transition Loss: 0.0602, Policy Loss: 0.0233\n",
            "Epoch 410, Transition Loss: 0.0830, Policy Loss: 0.0814\n",
            "Epoch 411, Transition Loss: 0.0533, Policy Loss: 0.1167\n",
            "Epoch 412, Transition Loss: 0.0549, Policy Loss: 0.0476\n",
            "Epoch 413, Transition Loss: 0.0480, Policy Loss: 0.0608\n",
            "Epoch 414, Transition Loss: 0.0869, Policy Loss: 0.0535\n",
            "Epoch 415, Transition Loss: 0.0808, Policy Loss: 0.0304\n",
            "Epoch 416, Transition Loss: 0.0511, Policy Loss: 0.0505\n",
            "Epoch 417, Transition Loss: 0.0528, Policy Loss: 0.0860\n",
            "Epoch 418, Transition Loss: 0.0444, Policy Loss: 0.0693\n",
            "Epoch 419, Transition Loss: 0.0720, Policy Loss: 0.0331\n",
            "Epoch 420, Transition Loss: 0.0447, Policy Loss: 0.0571\n",
            "Epoch 421, Transition Loss: 0.0658, Policy Loss: 0.0561\n",
            "Epoch 422, Transition Loss: 0.0566, Policy Loss: 0.1008\n",
            "Epoch 423, Transition Loss: 0.0310, Policy Loss: 0.0940\n",
            "Epoch 424, Transition Loss: 0.0593, Policy Loss: 0.0823\n",
            "Epoch 425, Transition Loss: 0.0323, Policy Loss: 0.1100\n",
            "Epoch 426, Transition Loss: 0.0677, Policy Loss: 0.0610\n",
            "Epoch 427, Transition Loss: 0.0393, Policy Loss: 0.0337\n",
            "Epoch 428, Transition Loss: 0.0386, Policy Loss: 0.0620\n",
            "Epoch 429, Transition Loss: 0.0797, Policy Loss: 0.0340\n",
            "Epoch 430, Transition Loss: 0.0844, Policy Loss: 0.0797\n",
            "Epoch 431, Transition Loss: 0.0623, Policy Loss: 0.0805\n",
            "Epoch 432, Transition Loss: 0.0818, Policy Loss: 0.0545\n",
            "Epoch 433, Transition Loss: 0.0459, Policy Loss: 0.0696\n",
            "Epoch 434, Transition Loss: 0.0472, Policy Loss: 0.0739\n",
            "Epoch 435, Transition Loss: 0.0590, Policy Loss: 0.0563\n",
            "Epoch 436, Transition Loss: 0.0518, Policy Loss: 0.0569\n",
            "Epoch 437, Transition Loss: 0.0623, Policy Loss: 0.0485\n",
            "Epoch 438, Transition Loss: 0.0368, Policy Loss: 0.0501\n",
            "Epoch 439, Transition Loss: 0.0659, Policy Loss: 0.0792\n",
            "Epoch 440, Transition Loss: 0.0662, Policy Loss: 0.0687\n",
            "Epoch 441, Transition Loss: 0.0441, Policy Loss: 0.0691\n",
            "Epoch 442, Transition Loss: 0.0482, Policy Loss: 0.0345\n",
            "Epoch 443, Transition Loss: 0.0447, Policy Loss: 0.0595\n",
            "Epoch 444, Transition Loss: 0.0610, Policy Loss: 0.0634\n",
            "Epoch 445, Transition Loss: 0.0662, Policy Loss: 0.0832\n",
            "Epoch 446, Transition Loss: 0.0510, Policy Loss: 0.0972\n",
            "Epoch 447, Transition Loss: 0.0613, Policy Loss: 0.0614\n",
            "Epoch 448, Transition Loss: 0.0547, Policy Loss: 0.0398\n",
            "Epoch 449, Transition Loss: 0.0630, Policy Loss: 0.0393\n",
            "Epoch 450, Transition Loss: 0.0580, Policy Loss: 0.0648\n",
            "Epoch 451, Transition Loss: 0.0433, Policy Loss: 0.0593\n",
            "Epoch 452, Transition Loss: 0.0638, Policy Loss: 0.0539\n",
            "Epoch 453, Transition Loss: 0.0372, Policy Loss: 0.0475\n",
            "Epoch 454, Transition Loss: 0.0646, Policy Loss: 0.0231\n",
            "Epoch 455, Transition Loss: 0.0623, Policy Loss: 0.0946\n",
            "Epoch 456, Transition Loss: 0.0596, Policy Loss: 0.0908\n",
            "Epoch 457, Transition Loss: 0.0484, Policy Loss: 0.0681\n",
            "Epoch 458, Transition Loss: 0.0883, Policy Loss: 0.0785\n",
            "Epoch 459, Transition Loss: 0.0566, Policy Loss: 0.0530\n",
            "Epoch 460, Transition Loss: 0.0603, Policy Loss: 0.0486\n",
            "Epoch 461, Transition Loss: 0.0427, Policy Loss: 0.1032\n",
            "Epoch 462, Transition Loss: 0.0567, Policy Loss: 0.0498\n",
            "Epoch 463, Transition Loss: 0.0759, Policy Loss: 0.0304\n",
            "Epoch 464, Transition Loss: 0.0638, Policy Loss: 0.0742\n",
            "Epoch 465, Transition Loss: 0.0373, Policy Loss: 0.0895\n",
            "Epoch 466, Transition Loss: 0.0703, Policy Loss: 0.0423\n",
            "Epoch 467, Transition Loss: 0.0521, Policy Loss: 0.1022\n",
            "Epoch 468, Transition Loss: 0.0394, Policy Loss: 0.0463\n",
            "Epoch 469, Transition Loss: 0.0438, Policy Loss: 0.0316\n",
            "Epoch 470, Transition Loss: 0.0792, Policy Loss: 0.0596\n",
            "Epoch 471, Transition Loss: 0.0803, Policy Loss: 0.0529\n",
            "Epoch 472, Transition Loss: 0.0663, Policy Loss: 0.0728\n",
            "Epoch 473, Transition Loss: 0.0621, Policy Loss: 0.0742\n",
            "Epoch 474, Transition Loss: 0.0497, Policy Loss: 0.0588\n",
            "Epoch 475, Transition Loss: 0.0707, Policy Loss: 0.0432\n",
            "Epoch 476, Transition Loss: 0.0684, Policy Loss: 0.0348\n",
            "Epoch 477, Transition Loss: 0.0406, Policy Loss: 0.0611\n",
            "Epoch 478, Transition Loss: 0.0776, Policy Loss: 0.0565\n",
            "Epoch 479, Transition Loss: 0.0539, Policy Loss: 0.0594\n",
            "Epoch 480, Transition Loss: 0.0396, Policy Loss: 0.0650\n",
            "Epoch 481, Transition Loss: 0.0620, Policy Loss: 0.0477\n",
            "Epoch 482, Transition Loss: 0.0579, Policy Loss: 0.0873\n",
            "Epoch 483, Transition Loss: 0.0538, Policy Loss: 0.0347\n",
            "Epoch 484, Transition Loss: 0.0491, Policy Loss: 0.0284\n",
            "Epoch 485, Transition Loss: 0.0622, Policy Loss: 0.0198\n",
            "Epoch 486, Transition Loss: 0.0709, Policy Loss: 0.0793\n",
            "Epoch 487, Transition Loss: 0.0615, Policy Loss: 0.0476\n",
            "Epoch 488, Transition Loss: 0.0526, Policy Loss: 0.0466\n",
            "Epoch 489, Transition Loss: 0.0504, Policy Loss: 0.0529\n",
            "Epoch 490, Transition Loss: 0.0458, Policy Loss: 0.0683\n",
            "Epoch 491, Transition Loss: 0.0505, Policy Loss: 0.0846\n",
            "Epoch 492, Transition Loss: 0.0490, Policy Loss: 0.0624\n",
            "Epoch 493, Transition Loss: 0.0602, Policy Loss: 0.0659\n",
            "Epoch 494, Transition Loss: 0.0789, Policy Loss: 0.0439\n",
            "Epoch 495, Transition Loss: 0.0351, Policy Loss: 0.0418\n",
            "Epoch 496, Transition Loss: 0.0416, Policy Loss: 0.0364\n",
            "Epoch 497, Transition Loss: 0.0455, Policy Loss: 0.0644\n",
            "Epoch 498, Transition Loss: 0.0630, Policy Loss: 0.0698\n",
            "Epoch 499, Transition Loss: 0.0412, Policy Loss: 0.0282\n",
            "Epoch 500, Transition Loss: 0.0384, Policy Loss: 0.0778\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'eval_env' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e5461579f674>\u001b[0m in \u001b[0;36m<cell line: 130>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Assuming eval_env and deployment_env are set up appropriately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mavg_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Average Reward in Evaluation: {avg_reward}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'eval_env' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# **Assumptions**:\n",
        "# - States and actions are continuous and normalized.\n",
        "# - Expert trajectories are available as `(state, action, next_state)` tuples.\n",
        "\n",
        "### 1. Dataset Class for Expert Trajectories\n",
        "class ExpertTrajectoryDataset(Dataset):\n",
        "    def __init__(self, states, actions, next_states):\n",
        "        self.states = torch.tensor(states, dtype=torch.float32)\n",
        "        self.actions = torch.tensor(actions, dtype=torch.float32)\n",
        "        self.next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.states)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.states[idx], self.actions[idx], self.next_states[idx]\n",
        "\n",
        "### 2. Forward (Transition) Model\n",
        "class TransitionModel(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super(TransitionModel, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, state_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, state_action):\n",
        "        return self.model(state_action)\n",
        "\n",
        "### 3. Reverse (Policy) Model\n",
        "class PolicyModel(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super(PolicyModel, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.model(state)\n",
        "\n",
        "### 4. Training\n",
        "def train(models, device, loader, optimizers, epochs=100):\n",
        "    transition_model, policy_model = models\n",
        "    transition_optimizer, policy_optimizer = optimizers\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in loader:\n",
        "            states, actions, next_states = [b.to(device) for b in batch]\n",
        "\n",
        "            # Forward (Transition) Model Training\n",
        "            transition_optimizer.zero_grad()\n",
        "            pred_next_states = transition_model(torch.cat((states, actions), dim=1))\n",
        "            loss_transition = ((pred_next_states - next_states) ** 2).mean()\n",
        "            loss_transition.backward()\n",
        "            transition_optimizer.step()\n",
        "\n",
        "            # Reverse (Policy) Model Training\n",
        "            # **Simplified Objective**: Minimize the difference between predicted actions and actual actions.\n",
        "            policy_optimizer.zero_grad()\n",
        "            pred_actions = policy_model(states)\n",
        "            loss_policy = ((pred_actions - actions) ** 2).mean()\n",
        "            loss_policy.backward()\n",
        "            policy_optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Transition Loss: {loss_transition.item():.4f}, Policy Loss: {loss_policy.item():.4f}\")\n",
        "\n",
        "### Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # **Mock Data**\n",
        "    np.random.seed(0)\n",
        "    states = np.random.rand(100, 5)  # Assuming 5D state space\n",
        "    actions = np.random.rand(100, 3)  # Assuming 3D action space\n",
        "    next_states = np.random.rand(100, 5)\n",
        "\n",
        "    # Setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    dataset = ExpertTrajectoryDataset(states, actions, next_states)\n",
        "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    transition_model = TransitionModel(state_dim=5, action_dim=3).to(device)\n",
        "\n",
        "    policy_model = PolicyModel(state_dim=5, action_dim=3).to(device)\n",
        "\n",
        "    transition_optimizer = optim.Adam(transition_model.parameters(), lr=0.001)\n",
        "    policy_optimizer = optim.Adam(policy_model.parameters(), lr=0.001)\n",
        "\n",
        "    train(models=(transition_model, policy_model),\n",
        "          device=device,\n",
        "          loader=loader,\n",
        "          optimizers=(transition_optimizer, policy_optimizer),\n",
        "          epochs=500)\n",
        "### Evaluation (Simplified)\n",
        "def evaluate_policy(policy_model, eval_env, device, num_episodes=100):\n",
        "    total_reward = 0\n",
        "    for _ in range(num_episodes):\n",
        "        state = eval_env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        while not done:\n",
        "            action = policy_model(torch.tensor(state, dtype=torch.float32).to(device))\n",
        "            action = action.cpu().detach().numpy()\n",
        "            state, reward, done, _ = eval_env.step(action)\n",
        "            episode_reward += reward\n",
        "        total_reward += episode_reward\n",
        "    avg_reward = total_reward / num_episodes\n",
        "    return avg_reward\n",
        "\n",
        "### Deployment (Conceptual)\n",
        "def deploy_policy(policy_model, deployment_env, device, episodes=1000):\n",
        "    for episode in range(episodes):\n",
        "        state = deployment_env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy_model(torch.tensor(state, dtype=torch.float32).to(device))\n",
        "            action = action.cpu().detach().numpy()\n",
        "            state, _, done, _ = deployment_env.step(action)\n",
        "            # Optionally, log or display the environment's response to the action\n",
        "        # Optionally, log or display episode end status\n",
        "\n",
        "### Example Usage for Evaluation and Deployment\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming eval_env and deployment_env are set up appropriately\n",
        "    avg_reward = evaluate_policy(policy_model, eval_env, device)\n",
        "    print(f\"Average Reward in Evaluation: {avg_reward}\")\n",
        "\n",
        "    deploy_policy(policy_model, deployment_env, device)\n",
        "\n"
      ]
    }
  ]
}